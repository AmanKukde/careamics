{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Noise2Void model\n",
    "\n",
    "Both the CARE network and Noise2Noise network you trained in part 1 and 2 require that you acquire additional data for the purpose of denoising. For CARE we used a paired acquisition with high SNR, for Noise2Noise we had paired noisy acquisitions. We will now train a Noise2Void network from single noisy images.\n",
    "\n",
    "This notebook uses a single image from the SEM data from the Noise2Noise notebook.\n",
    "\n",
    "We use the [Careamics](https://careamics.github.io) library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\\\"> \n",
    "Set your kernel to \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.2: Bla</h3>\n",
    "    \n",
    "This notebook uses a single image from the SEM data from the Noise2Noise notebook.\n",
    "\n",
    "If you brought your own raw data, use that instead! The only requirement is that the noise in your data is pixel-independent and zero-mean. If you're unsure whether your data fulfills that requirement or you don't yet understand why it is necessary ask one of us to discuss!\n",
    "\n",
    "If you don't have suitable data of your own, feel free to find some online or ask your fellow course participants. You can however also stick with the SEM data provided here and compare the results to what you achieved with Noise2Noise in the previous part.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "![diff_costs](static/diff_costs.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\"><div class=\"alert alert-block alert-success\"><h1>Checkpoint 1</h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "from careamics_portfolio import PortfolioManager\n",
    "\n",
    "from careamics import CAREamist\n",
    "from careamics.config import (\n",
    "    create_n2v_configuration,\n",
    ")\n",
    "from careamics.transforms import N2VManipulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Prepare the data\n",
    "\n",
    "For this we download the relevant dataset from the CAREamics portfolio library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore portfolio\n",
    "portfolio = PortfolioManager()\n",
    "print(portfolio.denoising)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files # TODO File should be reused from previous exercise\n",
    "root_path = Path(\"./data\")\n",
    "files = portfolio.denoising.N2V_SEM.download(root_path)\n",
    "print(f\"List of downloaded files: {files}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "train_image = tifffile.imread(files[0])\n",
    "print(f\"Train image shape: {train_image.shape}\")\n",
    "plt.imshow(train_image, cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image = tifffile.imread(files[1])\n",
    "print(f\"Validation image shape: {val_image.shape}\")\n",
    "plt.imshow(val_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "\n",
    "data_path = Path(root_path / \"n2v_sem\")\n",
    "train_path = data_path / \"train\"\n",
    "val_path = data_path / \"val\"\n",
    "\n",
    "train_path.mkdir(parents=True, exist_ok=True)\n",
    "val_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shutil.copy(root_path / files[0], train_path / \"train_image.tif\")\n",
    "shutil.copy(root_path / files[1], val_path / \"val_image.tif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Part 2: Learn about N2V masking</h3>\n",
    "\n",
    "To keep the network from learning the identity we have to manipulate the input pixels for the blindspot during training. How to exactly manipulate those values is controlled via the N2VManipulate and strategy to be specific. Default value of the masking strategy parameter is 'uniform' which samples a random value from the surrounding pixels, including or excluding the value at the control point. The size of the surrounding area can be configured via roi_size, which defines the side of a square region around the control point. \n",
    "\n",
    "The paper supplement describes other pixel manipulators as well (section 3.1). \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1 Visualize masking algorithm\n",
    "\n",
    "Let's start by looking into how Noise2Void masking works. The default masking strategy\n",
    "replaces certain percentage of pixel values with randomly selected values from the vicinity.\n",
    "\n",
    "In the next cell we'll define the parametes. Feel free to play around with the values to see how the masking changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masking parameters\n",
    "dummy_patch_size = 10\n",
    "roi_size = 5\n",
    "masked_pixel_percentage = 5\n",
    "strategy = 'uniform'\n",
    "\n",
    "# This is simply the index of a pixel to draw a rectangle around\n",
    "highlighted_pixel_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy patch\n",
    "patch = np.arange(dummy_patch_size**2).reshape(dummy_patch_size, dummy_patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the manipulator\n",
    "manipulator = N2VManipulate(\n",
    "    roi_size=roi_size,\n",
    "    masked_pixel_percentage=masked_pixel_percentage,\n",
    "    strategy=strategy,\n",
    ")\n",
    "\n",
    "# We need to add channel dimension because it's expected by the manipulator\n",
    "masked_patch, _, mask = manipulator.apply(patch[..., None])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper variables for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the masked pixels\n",
    "i, j = np.where(mask.squeeze())\n",
    "masked_pixel_coords = np.concatenate([i[:, None], j[:, None]], axis=1)\n",
    "\n",
    "# Get the coordinates of the one of the border pixels\n",
    "center_coords = np.flip(masked_pixel_coords[highlighted_pixel_idx]) - roi_size // 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the pixel with replaced values and the mask\n",
    "\n",
    "The cell below will visualize the patch with replaced values and the corresponding mask.\n",
    "You can see the size of the region of interest (roi_size) defined by the red square. The pixel in the center of the square is the one that was replaced with a random value from within the square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a roi mask to visualize the masked region\n",
    "roi = plt.Rectangle(center_coords, roi_size - 1, roi_size - 1, edgecolor='r', fill=False)\n",
    "\n",
    "# Visualize the masked patch and the mask\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(masked_patch)\n",
    "ax[0].add_patch(roi)\n",
    "ax[1].imshow(mask, cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Part 2.2: Create configuraion</h3>\n",
    "\n",
    "CAREamics can be configured either from a yaml file, or with an explicitly created config object.\n",
    "In this note book we will create the config object using the helper function. CAREamics will \n",
    "validate all the parameters and will output explicit error if some parameters or a combination of parameters isn't allowed. It will also provide default values for missing parameters. \n",
    "\n",
    "When creating the config-object, we provide the training data X. From X the library will extract mean and std that will be used to normalize all data before it is processed by the network.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size patch_size are extracted during training. Default patch shape is set to (64, 64).\n",
    "Parameters axes and data_type control the shape of data and the extension of the file.\n",
    "It's also possible to provide 'array' as data_type, in which case the data is expected to be a numpy array.\n",
    "\n",
    "Please take as look at the [documentation](https://careamics.github.io) to see the full list of parameters and configuration options\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2.2: Bla</h3>\n",
    "\n",
    "Take a look at the configuration parameters, make sure you understand what they do and \n",
    "try to run the training.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration using the helper function\n",
    "training_config = create_n2v_configuration(\n",
    "    experiment_name=\"LevitatingFrog\",\n",
    "    data_type=\"tiff\",\n",
    "    axes=\"YX\",\n",
    "    patch_size=[64, 64],\n",
    "    batch_size=128,\n",
    "    num_epochs=10,\n",
    "    roi_size=3,\n",
    "    masked_pixel_percentage=0.05,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Model\n",
    "\n",
    "Let's instantiate the model with the configuration we just created.\n",
    "\n",
    "CAREamist is the main umbrella class of the library, which will handle creation of the data pipeline, the model, training and inference methods.\n",
    "\n",
    "It's also possible to define all the components separately, which will allow for more fine-grained control over the data preparation and training processes. CAREamics is using Pytorch Lightning, so defining the pipeline will follow the regular Pytorch Lightning procedure.\n",
    "\n",
    "Please take as look at the [documentation](https://careamics.github.io) to see the full description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = CAREamist(source=training_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Run training \n",
    "\n",
    "We need to specify the paths to training and validation data. \n",
    "\n",
    "CAREamics can also accept an array or a list of arrays, in which case the data is expected to fit in memory. In case of paths or Pytorch Lightning datamodule data can be processed in memory or streamed from disk. \n",
    "\n",
    "Please take as look at the [documentation](https://careamics.github.io) to see the full description of supported data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.train(train_source=train_path, val_source=val_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Prediction\n",
    "\n",
    "For performing inference, we also need to specify the path and we can use the bigger tile size to make the process faster. \n",
    "\n",
    "By default CAREamics uses tiled prediction to handle large images. The tile size can be set via the tile_size parameter. Tile overlap is computed automatically based on the network architecture.\n",
    "\n",
    "To see the full description of the parameters and methods, please take a look at the [documentation](https://careamics.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = engine.predict(source=train_path, tile_size=(256, 256))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(train_image, cmap=\"gray\")\n",
    "ax[1].imshow(preds.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in\n",
    "\n",
    "y_start = 100\n",
    "y_end = 400\n",
    "x_start = 100\n",
    "x_end = 400\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\n",
    "ax[1].imshow(preds.squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3: Try to improve results</h3>\n",
    "\n",
    "CAREamics configuration won't allow you to use parameters which are clearly wrong. However, there are many parameters that can be tuned to improve the results. Try to play around with the roi_size and masked_pixel_percentage and see if you can improve the results.\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmcs_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
